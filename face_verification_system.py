# -*- coding: utf-8 -*-
"""Face verification system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hCLlGTGDcdNbMj1N4s0EeTIdY1yJgCUk

# Face verification + NAS + siamese network

# 1 image per class
"""

# upload the dataset to google
from google.colab import files
uploaded = files.upload()

#unzip the file
import zipfile

with zipfile.ZipFile("/content/face-verification.zip", 'r') as zip_ref:
  zip_ref.extractall("/content/data")

# pre-process all the images
import os
import cv2
import numpy as np

def preprocess_folder(folder_path, target_size=(105, 105)):
    """
    Preprocess all images inside a folder:
    1. Load each image
    2. Convert to RGB
    3. Resize to target size
    4. Normalize pixel values to [0, 1]
    5. Store all processed images in a list
    """
    processed_images = []
    image_names = []

    for filename in os.listdir(folder_path):
        if filename.endswith(".jpg") or filename.endswith(".png") or filename.endswith(".jpeg"):
            img_path = os.path.join(folder_path, filename)
            img = cv2.imread(img_path)

            if img is None:
                print(f"Warning: {filename} could not be read, skipping.")
                continue

            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = cv2.resize(img, target_size)
            img = img / 255.0

            processed_images.append(img)
            image_names.append(filename)

    return np.array(processed_images), image_names

folder_path = "/content/data/face-verification/"
images, names = preprocess_folder(folder_path)

print(f"Total images preprocessed: {len(images)}")
print(f"First image shape: {images[0].shape}")  # should be (105, 105, 3)
print(f"Names: {names}")  # file names like ['beee.jpg', 'alice.jpg']

# see first image
import matplotlib.pyplot as plt

plt.imshow(images[0])
plt.title(f"First image - {names[0]}")
plt.axis('off')
plt.show()

"""Generating Pairs"""

def create_full_pairs(images, names):
    """
    Create ALL possible positive and negative pairs.
    Positive: same person (self-pair)
    Negative: different persons (cross-pair)
    """

    pairs = []
    labels = []

    num_images = len(images)

    for i in range(num_images):
        # Positive pair (same image with itself)
        img1 = images[i]
        img2 = images[i]
        pairs.append([img1, img2])
        labels.append(0)

        # Negative pairs (with every other different image)
        for j in range(num_images):
            if i != j:
                img1 = images[i]
                img2 = images[j]
                pairs.append([img1, img2])
                labels.append(1)

    return np.array(pairs), np.array(labels)

pairs, labels = create_full_pairs(images, names)

print(f"Total pairs created: {len(pairs)}")
print(f"Example - Label {labels[0]} â†’ Same person" if labels[0]==0 else "Different people")

import matplotlib.pyplot as plt
import random

def show_random_pairs(pairs, labels, num=5):
    plt.figure(figsize=(10, 4))

    for i in range(num):
        idx = random.randint(0, len(pairs)-1)
        img1, img2 = pairs[idx]

        plt.subplot(2, num, i+1)
        plt.imshow(img1)
        plt.title("Image 1")
        plt.axis('off')

        plt.subplot(2, num, i+1+num)
        plt.imshow(img2)
        label = "Same" if labels[idx] == 0 else "Different"
        plt.title(f"Image 2\n{label}")
        plt.axis('off')

    plt.show()

show_random_pairs(pairs, labels)

"""**Siamese network**

* Build a shared CNN branch for feature extraction

* Both images will pass through the same CNN

* This CNN will convert a face image into a compact feature vector embeddings
"""

from collections.abc import AsyncIterable
import tensorflow as tf
from tensorflow.keras import layers, models, Sequential

# Build the feature extractor CNN
embedding_model = models.Sequential([
    layers.Conv2D(64, (3,3), activation='relu', input_shape=(105,105, 3)),
    layers.MaxPooling2D(),

    layers.Conv2D(128, (3,3), activation='relu'),
    layers.MaxPooling2D(),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(64, activation='relu')
])

embedding_model.summary()

# prepare two inputs
input_1 = layers.Input(shape=(105,105,3)) # first face image
input_2 = layers.Input(shape=(105,105,3)) # second face image

# pass both inputs through the same embedding model
embedding_1 = embedding_model(input_1)
embedding_2 = embedding_model(input_2)

# compute the distance between two embeddings
distance = layers.Lambda(
    lambda tensors: tf.abs(tensors[0] - tensors[1]),
    output_shape=(64,)   # ðŸ‘ˆ ADD THIS
)([embedding_1, embedding_2])

# Final Decision Layer
output = layers.Dense(1, activation='sigmoid')(distance)

# Full siamese model
siamese_model = models.Model(inputs=[input_1, input_2], outputs=output)

siamese_model.summary()

"""**ðŸ§  Siamese Network Architecture**

```
Image 1 -----> Shared CNN ------> Embedding 1
                                  â†˜
                                   |-> |Embedding 1 - Embedding 2| --> Dense --> Prediction (Same/Different)
                                  â†—
Image 2 -----> Shared CNN ------> Embedding 2
```

"""

# compile the model
siamese_model.compile(
    loss='binary_crossentropy', #because output is 0/1 (same/different)
    optimizer='adam',
    metrics=['accuracy']
)

# Train the model
history = siamese_model.fit(
    [pairs[:,0], pairs[:,1]], #pass both image inputs separately
    labels, # Labels:0(same) or 1(different)
    batch_size=16,
    epochs=50,
    validation_split=0.2
)

# training graph
plt.plot(history.history['accuracy'], label='train accuracy')
plt.plot(history.history['val_accuracy'], label='validaiton accuracy')

# Loss graph
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')

# save the siamese model
siamese_model.save("/content/siamese_model.keras")
print("âœ… Baseline Siamese model saved in .keras format.")

# save the face embeddings
import os
import pickle
import cv2
import numpy as np

# Path to registered faces
registered_faces_folder = "/content/data/face-verification/"

# Load all registered faces, preprocess, and extract embeddings
embeddings = {}
for filename in os.listdir(registered_faces_folder):
    if filename.endswith(".jpg") or filename.endswith(".png") or filename.endswith(".jpeg"):
        img_path = os.path.join(registered_faces_folder, filename)

        # Preprocess
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (105, 105))
        img = img / 255.0
        img = img.astype('float32')
        img = np.expand_dims(img, axis=0)  # (1, 105, 105, 3)

        # Get the embedding vector
        embedding_vector = embedding_model.predict(img)[0]  # output shape (64,)

        # Save name and embedding
        name = filename.split(".")[0]
        embeddings[name] = embedding_vector

# Save all embeddings into a pickle file
with open("/content/face-embeddings.pkl", "wb") as f:
    pickle.dump(embeddings, f)

print("âœ… Embeddings saved successfully.")

# see the face embeddings
import pickle

# Load the saved embeddings file
with open("/content/face-embeddings.pkl", "rb") as f:
    saved_embeddings = pickle.load(f)

# Show the names stored
print(f"Registered Faces: {list(saved_embeddings.keys())}")

# Show the embedding vector for one person (for example: bellam)
print("\nEmbedding vector for 'bellam':")
print(saved_embeddings['bellam'])
print(f"Shape of embedding: {saved_embeddings['bellam'].shape}")

"""**NAS - Neural architecture search**"""

# Search Space
search_space = {
    "num_conv_layers": [2,3], # 2 or 3 convolutional layers
    "filters": [32,64,128], #Number of filters to try
    "kernel_sizes": [3,5], #3*3 or 5*5 kernels
    "dense_units": [32,64,128], #dense layers (after flatten)
    "num_dense_layers": [2], # you can choose 2,3,4,5,6, anything
    "activations": ['relu', 'leaky_relu']
}

# Search strategy
import itertools

# create all possible combinations
all_combinations = list(itertools.product(
    search_space['num_conv_layers'],
    search_space['filters'],
    search_space['kernel_sizes'],
    search_space['dense_units'],
    search_space['num_dense_layers'],
    search_space['activations']
))
print(f"Total combinations to try: {len(all_combinations)}")

# ðŸ§± Build Dynamic Growing CNN Function
def build_dynamic_cnn(sampled_num_conv_layers, sampled_start_filters, sampled_kernel_size,
                      sampled_start_dense_units, sampled_num_dense_layers, sampled_activation):
    input_layer = layers.Input(shape=(105, 105, 3))
    x = input_layer
    filters = sampled_start_filters

    for i in range(sampled_num_conv_layers):
        x = layers.Conv2D(filters=filters, kernel_size=(sampled_kernel_size, sampled_kernel_size), activation=sampled_activation)(x)
        x = layers.MaxPooling2D()(x)
        filters *= 2

    x = layers.Flatten()(x)
    dense_units = sampled_start_dense_units

    for i in range(sampled_num_dense_layers):
        x = layers.Dense(units=dense_units, activation=sampled_activation)(x)
        dense_units = max(8, dense_units // 2)

    embedding_output = x
    model = models.Model(inputs=input_layer, outputs=embedding_output)
    return model

# ðŸ”µ Setup Grid Search Loop
N_TRIALS = 10  # how many models to try

results = []

for i, combo in enumerate(all_combinations[:N_TRIALS]):
    print(f"\nðŸ”µ Trying architecture {i+1}/{N_TRIALS}")

    sampled_num_conv_layers = combo[0]
    sampled_start_filters = combo[1]
    sampled_kernel_size = combo[2]
    sampled_start_dense_units = combo[3]
    sampled_num_dense_layers = combo[4]
    sampled_activation = combo[5]

    print(f"Sampled Architecture:")
    print(f"Conv Layers: {sampled_num_conv_layers}, Filters: {sampled_start_filters}, Kernel Size: {sampled_kernel_size}")
    print(f"Dense Units: {sampled_start_dense_units}, Dense Layers: {sampled_num_dense_layers}, Activation: {sampled_activation}")

    # ðŸš€ Build the model dynamically
    model = build_dynamic_cnn(sampled_num_conv_layers, sampled_start_filters,
                              sampled_kernel_size, sampled_start_dense_units,
                              sampled_num_dense_layers, sampled_activation)

    # ðŸš€ (Next we will compile and train here)

model = build_dynamic_cnn(sampled_num_conv_layers, sampled_start_filters,
                          sampled_kernel_size, sampled_start_dense_units,
                          sampled_num_dense_layers, sampled_activation)

# Show the real model structure
model.summary()

# Define the output layer
# Build the embedding model (shared CNN part)
embedding_model = build_dynamic_cnn(sampled_num_conv_layers, sampled_start_filters,
                                    sampled_kernel_size, sampled_start_dense_units,
                                    sampled_num_dense_layers, sampled_activation)

# Add final output head (sigmoid classifier)
final_output = layers.Dense(1, activation='sigmoid')(embedding_model.output)

# Create final model (Siamese output model)
final_model = models.Model(inputs=embedding_model.input, outputs=final_output)

final_model.summary()

# Evaluation strategy
# save the best validation accuracy
best_val_accuracy = max(history.history['val_accuracy'])

# compile the model
final_model.compile(
    optimizer='adam',
    loss = 'binary_crossentropy',
    metrics=['accuracy']
)

print(X.shape)
print(X.dtype)
print(X.min(), X.max())

for i, combo in enumerate(all_combinations[:N_TRIALS]):

    from tensorflow.keras import backend as K
    K.clear_session()  # ðŸ§¹ CLEAR SESSION FIRST

    # 1. Sample architecture (you already doing)
    # 2. Build model
    # 3. Compile model
    # 4. Train model
    # 5. Save best val_accuracy

# clean imports
import numpy as np
import tensorflow as tf

# Clean images
X = np.array(images).astype('float32') / 255.0

# Dummy binary labels
y = np.random.randint(0, 2, size=(len(X),))

# (NO tf.data.Dataset - remove it!)

# Compile model again fresh
final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# DIRECTLY train
history = final_model.fit(
    X,    # input images (pure np.array)
    y,    # labels (binary 0/1)
    batch_size=2,
    epochs=10,
    verbose=1
)

# Save NAS best model
final_model.save('/content/nas_best_model.h5')



"""Load and test the model"""

import keras

# Enable unsafe deserialization for loading Lambda layers
keras.config.enable_unsafe_deserialization()

import tensorflow as tf

# Load the saved Siamese model
siamese_model = tf.keras.models.load_model('/content/siamese_model.keras')

print("âœ… Siamese model loaded successfully.")

import pickle

#Load the saved face embeddings
with open("/content/face-embeddings.pkl", "rb") as f:
  embeddings = pickle.load(f)

print(f"âœ… Loaded {len(embeddings)} face embeddings.")

# Load and pre-process new image
import cv2
import numpy as np

#path to new face image
new_image_path = "/content/chaitanya-test.jpg"

#pre=process function
def preprocess_image(img_path):
  img = cv2.imread(img_path)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  img = cv2.resize(img, (105,105))
  img = img / 255.0
  img = img.astype('float32')
  img = np.expand_dims(img, axis=0)
  return img

# preprocess your new image
new_face_img = preprocess_image(new_image_path)

print("âœ… New test face preprocessed and ready.")
print(f"Shape: {new_face_img.shape}")  # Should be (1, 105, 105, 3)

# Extract new face embedding
new_face_embedding = embedding_model.predict(new_face_img)[0] # (64,)

print("âœ… New face embedding extracted.")
print(f"Shape: {new_face_embedding.shape}")

# compare new face embedding with saved dataset embeddings
from scipy.spatial.distance import cosine

#set a threshold for deciding if the person matches
threshold = 0.5 # we can tune this later

#Initialize variables
min_distance = float('inf')
matched_name = None

#Compare new embedding with all saved embeddings
for name, saved_embedding in embeddings.items():
  distance = cosine(new_face_embedding, saved_embedding)
  if distance < min_distance:
    min_distance = distance
    matched_name = name

#Final decision
print(f"Minimum Distance: {min_distance:4f}")

if min_distance < threshold:
  print(f"âœ… ACCESS GRANTED: Welcome {matched_name}!")
else:
  print("âŒ ACCESS DENIED: Unknown Person.")

# Pipeline
import cv2
import numpy as np
import pickle
from tensorflow import keras
from scipy.spatial.distance import cosine
from google.colab import files

# Step 1: Load Model and Embeddings
keras.config.enable_unsafe_deserialization()

siamese_model = keras.models.load_model('/content/siamese_model.keras', safe_mode=False)

# Load embedding model separately if needed
embedding_model = siamese_model.layers[2]  # If sequential is the 3rd layer

with open("/content/face-embeddings.pkl", "rb") as f:
    embeddings = pickle.load(f)

print("âœ… Siamese model and embeddings loaded successfully.")

# Step 2: Ask user to upload image
uploaded = files.upload()

# Step 3: Preprocess uploaded image
def preprocess_image(img_path):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (105, 105))
    img = img / 255.0
    img = img.astype('float32')
    img = np.expand_dims(img, axis=0)
    return img

# Grab the uploaded file name
uploaded_image_path = list(uploaded.keys())[0]
new_face_img = preprocess_image(uploaded_image_path)

# Step 4: Extract embedding
new_face_embedding = embedding_model.predict(new_face_img)[0]

# Step 5: Compare with saved embeddings
threshold = 0.5  # Tune if needed
min_distance = float('inf')
matched_name = None

for name, saved_embedding in embeddings.items():
    distance = cosine(new_face_embedding, saved_embedding)
    if distance < min_distance:
        min_distance = distance
        matched_name = name

# Step 6: Final Decision
print(f"\nðŸ”µ Minimum Distance: {min_distance:.4f}")

if min_distance < threshold:
    print(f"âœ… ACCESS GRANTED: Welcome {matched_name}!")
else:
    print("âŒ ACCESS DENIED: Unknown Person.")



"""# 5 images per class"""

# upload the dataset to google
from google.colab import files
uploaded = files.upload()

#unzip the file
import zipfile

with zipfile.ZipFile("/content/Face-verification-5-10.zip", 'r') as zip_ref:
  zip_ref.extractall("/content/data")

# pre-process all the images
# Preprocess all images (going inside class folders)
import os
import cv2
import numpy as np

def preprocess_folder(folder_path, target_size=(224,224)):
    """
    Preprocess all images inside the dataset:
    1. Go inside each person folder
    2. Load each image
    3. Convert to RGB
    4. Resize to target size
    5. Normalize pixel values to [0, 1]
    6. Store all processed images and their labels
    """
    processed_images = []
    image_labels = []

    for person_name in os.listdir(folder_path):
        person_folder = os.path.join(folder_path, person_name)

        if os.path.isdir(person_folder):
            for filename in os.listdir(person_folder):
                if filename.endswith(".jpg") or filename.endswith(".png") or filename.endswith(".jpeg"):
                    img_path = os.path.join(person_folder, filename)
                    img = cv2.imread(img_path)

                    if img is None:
                        print(f"âš ï¸ Warning: {img_path} could not be read, skipping.")
                        continue

                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    img = cv2.resize(img, target_size)
                    img = img / 255.0

                    processed_images.append(img)
                    image_labels.append(person_name)  # Save label as folder name

    return np.array(processed_images), np.array(image_labels)

folder_path = "/content/data/Face-verification-5-10"
images, names = preprocess_folder(folder_path)

print(f"Total images preprocessed: {len(images)}")
print(f"First image shape: {images[0].shape}")  # should be (224, 224, 3)
print(f"Names: {names}")  # file names like ['beee.jpg', 'alice.jpg']

# create pairs
import numpy as np
import random

def create_pairs(images, labels, num_positive_per_class=5, num_negative_per_class=20):
    """
    Create positive and negative pairs from images and labels.

    Args:
        images: numpy array of shape (N, 105, 105, 3)
        labels: numpy array of shape (N,)
        num_positive_per_class: number of positive pairs per person
        num_negative_per_class: number of negative pairs per person

    Returns:
        pairs: numpy array of pairs (shape: (N_pairs, 2, 105, 105, 3))
        pair_labels: numpy array of labels (0 = same, 1 = different)
    """

    pairs = []
    pair_labels = []

    label_to_indices = {}
    for idx, label in enumerate(labels):
        if label not in label_to_indices:
            label_to_indices[label] = []
        label_to_indices[label].append(idx)

    unique_labels = list(label_to_indices.keys())

    # Create positive pairs
    for label in unique_labels:
        indices = label_to_indices[label]
        for _ in range(num_positive_per_class):
            i1, i2 = np.random.choice(indices, 2, replace=False)
            pairs.append([images[i1], images[i2]])
            pair_labels.append(0)  # same class => label 0

    # Create negative pairs
    for label in unique_labels:
        for _ in range(num_negative_per_class):
            label_a = label
            label_b = random.choice([l for l in unique_labels if l != label_a])

            i1 = random.choice(label_to_indices[label_a])
            i2 = random.choice(label_to_indices[label_b])

            pairs.append([images[i1], images[i2]])
            pair_labels.append(1)  # different class => label 1

    return np.array(pairs), np.array(pair_labels)

# Create pairs now
pairs, pair_labels = create_pairs(images, names, num_positive_per_class=5, num_negative_per_class=20)

print(f"Total pairs created: {len(pairs)}")
print(f"Positive examples: {(pair_labels == 0).sum()}")
print(f"Negative examples: {(pair_labels == 1).sum()}")

# Visualize some pairs
import matplotlib.pyplot as plt

def show_random_pairs(pairs, pair_labels, num=5):
    """
    Show random pairs of images with their label (Same or Different)
    """
    plt.figure(figsize=(10, 4 * num))

    for i in range(num):
        idx = random.randint(0, len(pairs)-1)
        img1, img2 = pairs[idx]

        label = pair_labels[idx]
        title = "Same Person" if label == 0 else "Different Persons"

        # Plot first image
        plt.subplot(num, 2, 2*i+1)
        plt.imshow(img1)
        plt.title("Image 1")
        plt.axis('off')

        # Plot second image
        plt.subplot(num, 2, 2*i+2)
        plt.imshow(img2)
        plt.title(f"Image 2\n({title})")
        plt.axis('off')

    plt.tight_layout()
    plt.show()

show_random_pairs(pairs, pair_labels, num=15)  # you can change num=10, 15 etc



"""**MobileNetV2 - 128 face embeddings**"""

!pip install keras-cv --upgrade

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models

# 1. Load MobileNetV2 backbone (pretrained on ImageNet)
backbone = keras.applications.MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,
    weights="imagenet",
)

# 2. Freeze backbone layers
for layer in backbone.layers:
    layer.trainable = False

print("âœ… Backbone frozen!")

# 3. Create Embedding model
embedding_model = models.Sequential([
    backbone,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128)  # 128-dimensional embedding
])

print("âœ… Embedding model ready!")
print("Embedding output shape:", embedding_model.output_shape)

embedding_model.summary()

# Siamese network
from tensorflow.keras import Input, Model

# 1. Inputs
input_1 = Input(shape=(224, 224, 3))
input_2 = Input(shape=(224, 224, 3))

# 2. Embeddings
embedding_1 = embedding_model(input_1)
embedding_2 = embedding_model(input_2)

# 3. Distance with output_shape
distance = layers.Lambda(
    lambda tensors: tf.abs(tensors[0] - tensors[1]),
    output_shape=(128,)
)([embedding_1, embedding_2])

# 4. Final prediction
output = layers.Dense(1, activation="sigmoid")(distance)

# 5. Build model
siamese_model = Model(inputs=[input_1, input_2], outputs=output)

# 6. Compile
siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

print("âœ… Siamese model rebuilt successfully with output shape!")

# Separate left and right images from pairs
left_images = pairs[:, 0]
right_images = pairs[:, 1]

# Train Siamese model
history = siamese_model.fit(
    [left_images, right_images],  # two inputs
    pair_labels,                  # labels (0 = same, 1 = different)
    batch_size=16,
    epochs=25,                    # fewer epochs now
    validation_split=0.2,
    verbose=1
)

# training graph
plt.plot(history.history['accuracy'], label='train accuracy')
plt.plot(history.history['val_accuracy'], label='validaiton accuracy')

# Loss graph
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='validaiton loss')

# Save Siamese model
siamese_model.save("/content/mobilenetv2_siamese_model.keras")
print("âœ… Siamese model saved successfully!")

import os
import pickle
import cv2
import numpy as np

# Path to your registered faces
registered_faces_folder = "/content/data/Face-verification-5-10/"  # update if different

# Save embeddings dictionary
embeddings = {}

for folder_name in os.listdir(registered_faces_folder):
    person_folder = os.path.join(registered_faces_folder, folder_name)

    if os.path.isdir(person_folder):
        for filename in os.listdir(person_folder):
            if filename.endswith(".jpg") or filename.endswith(".png") or filename.endswith(".jpeg"):
                img_path = os.path.join(person_folder, filename)

                # Preprocess image
                img = cv2.imread(img_path)
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                img = cv2.resize(img, (224, 224))
                img = img / 255.0
                img = img.astype('float32')
                img = np.expand_dims(img, axis=0)

                # Get embedding
                embedding_vector = embedding_model.predict(img)[0]

                # Save embedding
                name = f"{folder_name}/{filename}"
                embeddings[name] = embedding_vector

# Save to a pickle file
with open("/content/face_embeddings-5-10.pkl", "wb") as f:
    pickle.dump(embeddings, f)

print("âœ… Face embeddings saved successfully!")

import numpy as np
import cv2
import pickle
from scipy.spatial.distance import cosine
from tensorflow import keras
from tensorflow.keras import backend as K

# 1. Define the lambda function manually
def abs_diff(tensors):
    x, y = tensors
    return K.abs(x - y)

# 2. Load Siamese model with custom_objects
siamese_model = keras.models.load_model(
    '/content/mobilenetv2_siamese_model.keras',
    custom_objects={'<lambda>': abs_diff}
)
print("âœ… Siamese model loaded successfully.")

# 3. Load saved embeddings
with open("/content/face_embeddings-5-10.pkl", "rb") as f:
    saved_embeddings = pickle.load(f)
print(f"âœ… {len(saved_embeddings)} embeddings loaded.")

# 4. Upload new image
from google.colab import files
uploaded = files.upload()

# 5. Preprocess uploaded image
new_img_path = list(uploaded.keys())[0]
img = cv2.imread(new_img_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img = cv2.resize(img, (224, 224))
img = img / 255.0
img = img.astype('float32')
img = np.expand_dims(img, axis=0)

# 6. Get embedding of new image
new_embedding = embedding_model.predict(img)[0]

# 7. Compare with all saved embeddings
min_distance = float('inf')
matched_person = None

for name, saved_emb in saved_embeddings.items():
    distance = cosine(new_embedding, saved_emb)
    if distance < min_distance:
        min_distance = distance
        matched_person = name

# 8. Final decision
threshold = 0.7  # tune if needed

print("\nâœ… Final Result:")
if min_distance < threshold:
    print(f"âœ… ACCESS GRANTED to {matched_person}!")
    print(f"Distance: {min_distance:.4f}")
else:
    print("âŒ ACCESS DENIED. Unknown person.")
    print(f"Distance: {min_distance:.4f}")

